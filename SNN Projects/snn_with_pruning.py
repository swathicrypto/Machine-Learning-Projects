# -*- coding: utf-8 -*-
"""SNN_with_pruning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ImNRfyXGkNMU1AzyhyE33Luf0u3hkmpu
"""

from google.colab import drive
drive.mount('/content/drive')

cd drive/MyDrive/Iknur_work/

!pip install snntorch==0.7.0

!pip install binary_fractions

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader,TensorDataset
from torchvision import datasets, transforms

def data_loader(dataset = "mnist", in_shape = (28, 28), batch_size = 128):

    dtype = torch.float
    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

    # Define a transform
    transform = transforms.Compose([
                transforms.Resize(in_shape),
                transforms.Grayscale(),
                transforms.ToTensor(),
                transforms.Normalize((0,), (1,))])

    if dataset.lower() == 'mnist':
        data_path = './dataset/mnist'
        mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)
        mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)
        train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)
        test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)
    elif dataset.lower() == 'fashion-mnist':
        data_path = './dataset/fashion-mnist'
        fmnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transform)
        fmnist_test  = datasets.FashionMNIST(data_path, train=False, download=True, transform=transform)
        train_loader = DataLoader(fmnist_train, batch_size=batch_size, shuffle=True, drop_last=True)
        test_loader = DataLoader(fmnist_test, batch_size=batch_size, shuffle=True, drop_last=True)
    elif dataset.lower() == 'cifar10':
        data_path = './dataset/cifar10'
        cifar10_train = datasets.CIFAR10(data_path, train=True, download=True, transform=transform)
        cifar10_test  = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)
        train_loader = DataLoader(cifar10_train, batch_size=batch_size, shuffle=True, drop_last=True)
        test_loader = DataLoader(cifar10_test, batch_size=batch_size, shuffle=True, drop_last=True)

    return train_loader,test_loader

dataset = "fashion-mnist"
train_loader,test_loader = data_loader(dataset = dataset, in_shape = (28, 28), batch_size = 128)

import snntorch as snn
from snntorch import spikeplot as splt
from snntorch import spikegen
import snntorch.utils as utils
import torch
import torch.nn as nn

def print_batch_accuracy(net, batch_size, data, targets, train=False):
    output, _ = net(data)
    _, idx = output.sum(dim=0).max(1)
    acc = np.mean((targets == idx).detach().cpu().numpy())

    if train:
        print(f"Train set accuracy for a single minibatch: {acc*100:.2f}%")
    else:
        print(f"Test set accuracy for a single minibatch: {acc*100:.2f}%")

def train_printer(
    net, batch_size,
    data, targets, epoch,
    counter, iter_counter,
        loss_hist, test_loss_hist, test_data, test_targets):
    print(f"Epoch {epoch}, Iteration {iter_counter}")
    print(f"Train Set Loss: {loss_hist[counter]:.2f}")
    print(f"Test Set Loss: {test_loss_hist[counter]:.2f}")
    print('Average Test Loss: ',np.mean(test_loss_hist))
    print_batch_accuracy(net, batch_size, data, targets, train=True)
    print_batch_accuracy(net, batch_size, test_data, test_targets, train=False)
    print("\n")

# Define Network
class Net(nn.Module):
    def __init__(self,
                 num_inputs=1,
                 num_hidden_layers=1,
                 num_hidden=1,
                 num_outputs=1,
                 num_steps=10,
                 beta=0.8,
                 vth=1.0,
                 rst='zero',
                 bias=False):
        super().__init__()
        # Num steps
        self.num_steps = num_steps

        # Initialize layers
        # Generate the model architecture to be simulated
        arch = [num_inputs]
        for l in range(num_hidden_layers):
            arch.append(num_hidden)
        arch.append(num_outputs)
        # Number of layers
        self.num_layers = len(arch) - 1

        # Initialize Memory
        self.init_mems = list()

        # Define Layers
        self.layers = nn.ModuleList()
        for i in range(self.num_layers):
            fc  = nn.Linear(arch[i], arch[i+1], bias=bias)
            lif = snn.Leaky(beta=beta,threshold=vth,reset_mechanism=rst)
            mem = lif.init_leaky()
            self.layers.append(fc)
            self.layers.append(lif)
            self.init_mems.append(mem)

    def forward(self, x):
        mems = [self.layers[2*i+1].init_leaky() for i in range(self.num_layers)]

        # Record the final layer
        spk_out = []
        mem_out = []

        for step in range(self.num_steps):
            spk = x[step]
            for i in range(self.num_layers):
                cur = self.layers[2*i](spk)
                spk,mems[i] = self.layers[2*i+1](cur,mems[i])

            spk_out.append(spk)
            mem_out.append(mems[-1])

        return torch.stack(spk_out, dim=0), torch.stack(mem_out, dim=0)

import torch.nn.utils.prune as prune

for sp in range(10,51,10):

  print(f"Sparsity {sp}%", "\n\n")
  num_inputs = 28 * 28
  num_hidden = 256
  num_outputs = 10
  num_hidden_layers = 2

  dtype = torch.float
  device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

  # Load the network onto CUDA if available
  net = Net(num_inputs=num_inputs,
            num_hidden_layers=num_hidden_layers,
            num_hidden=num_hidden,
            num_outputs=num_outputs,
            num_steps=100,
            beta=0.95,
            vth=1.0,
            rst="subtract").to(device)
  # Loss and optimizer
  loss = nn.CrossEntropyLoss()
  optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))

  # Main training framework starts here
  utils.reset(net)
  loss_hist = []
  test_loss_hist = []
  counter = 0
  weight_scaling = True
  num_epochs = 5
  batch_size =128
  num_steps = 100

  # Pruning Info
  parameters_to_prune = []
  for name,mod in list(net.named_modules())[1:]:

      keys = dict(mod.named_parameters()).keys()

      if dict().keys() == keys:
          continue
      else:
          if 'bias' in keys:
              parameters_to_prune.append((mod,'bias'))

          if 'weight' in keys:
              parameters_to_prune.append((mod,'weight'))

  print("Number of layers to prune: ", len(parameters_to_prune), "\n")

  sparsity_amount = sp
  percentage_amount = (sparsity_amount/100)
  pruning_amount = [percentage_amount  for i in range(len(parameters_to_prune))]
  print(pruning_amount, "\n\n")

  for epoch in range(num_epochs):
      iter_counter = 0
      train_batch = iter(train_loader)

      for data, targets in train_batch:
          data = spikegen.rate(data.view(batch_size,-1),num_steps=num_steps).to(device)
          targets = targets.to(device)
          # forward pass
          net.train()
          spk_rec, mem_rec = net(data)

          # initialize the loss & sum over time
          loss_val = torch.zeros((1), dtype=dtype, device=device)
          for step in range(num_steps):
              loss_val += loss(mem_rec[step], targets)

          # Gradient calculation + weight update
          optimizer.zero_grad()
          loss_val.backward()
          optimizer.step()
          # Update weights
          for model_params in net.parameters():
              if weight_scaling:
                  model_params.data = torch.clamp(model_params.data,min=-0.9375,max=0.9375)

          torch.cuda.empty_cache()

          for j,i in enumerate(parameters_to_prune):
              modu,parm = i
              try:
                  prune.l1_unstructured(modu, name=parm, amount= pruning_amount[j])
                  prune.remove(modu, parm)
              except Exception:
                  pass

          # Store loss history for future plotting
          loss_hist.append(loss_val.item())
          # For inference
          # batch_infer = list()
          # Test set
          with torch.no_grad():
              net.eval()
              test_data, test_targets = next(iter(test_loader))
              test_data = spikegen.rate(test_data.view(batch_size,-1),num_steps=num_steps).to(device)
              #test_data = test_data.to(device)
              test_targets = test_targets.to(device)
              # Test set forward pass
              test_spk, test_mem = net(test_data)
              # Test set loss
              test_loss = torch.zeros((1), dtype=dtype, device=device)
              for step in range(num_steps):
                  test_loss += loss(test_mem[step], test_targets)
              test_loss_hist.append(test_loss.item())
              # Print train/test loss/accuracy
              if counter % 50 == 0:
                  train_printer(
                      net, batch_size,
                      data, targets, epoch,
                      counter, iter_counter,
                      loss_hist, test_loss_hist,
                      test_data, test_targets)
              counter += 1
              iter_counter +=1


  print("\n\n")

  # Evaluation
  pred = []
  target = []
  net.eval()
  for test_data, test_targets in test_loader:
    test_data = spikegen.rate(test_data.view(batch_size,-1),num_steps=num_steps).to(device)
    test_targets = test_targets
    output, _ = net(test_data)
    _, idx    = output.sum(dim=0).max(1)

    pred += idx.detach().cpu().numpy().tolist()
    target += test_targets.numpy().tolist()

  acc = np.mean((np.array(target) == np.array(pred)))
  print(f"Test batch accuracy for {sparsity_amount}% sparsity : {acc*100:.2f}%")
  print("\n\n")

  dict_name = dataset + "_784_256_256_10" + "_with_" + str(sparsity_amount) + "%_Sparsity" + '.dict'
  model_name = dataset + "_784_256_256_10" + "_with_" + str(sparsity_amount) + "%_Sparsity" +'.mdl'
  # Save output
  torch.save(net,model_name)
  torch.save(net.state_dict(),dict_name)



"""# To check sparsity"""

w = torch.load("MNIST/mnist_784_256_256_10" + "_with_" + str(20) + "%_Sparsity" + '.dict', map_location=torch.device('cpu'))

w.keys()

totParams = 0
sparseParams = 0
for ix,i in enumerate([w['layers.0.weight'], w['layers.2.weight'], w['layers.4.weight']]):
  var = (i !=0).sum()
  print("Linear Layer ",ix,": ",var, i.shape, "Params :", i.shape[0] * i.shape[1])
  sparseParams += var.item()
  totParams += i.shape[0] * i.shape[1]

print(f"\nTotal Params : {totParams}")
print(f"Sparse Params : {sparseParams}")

# Sparsity

print(f"Sparsity : {(1 - sparseParams/totParams)*100:.2f}%")