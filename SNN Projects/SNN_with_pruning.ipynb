{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0secLNKo-ucU",
        "outputId": "26727ae1-258c-45a8-9d99-c475f6d35f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQj6hlf5XpYo",
        "outputId": "c50e3ef4-d64d-4529-da9f-9bea13c85f2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1Dr0AezF1S8Tqp3hkotrq7lGVbj2ZF9Fu/Iknur_work\n"
          ]
        }
      ],
      "source": [
        "cd drive/MyDrive/Iknur_work/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snntorch==0.7.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yabBOOCCZKIu",
        "outputId": "0bfb1c34-13d8-4662-d650-c4674ae3d81d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snntorch==0.7.0\n",
            "  Downloading snntorch-0.7.0-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from snntorch==0.7.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from snntorch==0.7.0) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from snntorch==0.7.0) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from snntorch==0.7.0) (1.25.2)\n",
            "Collecting nir (from snntorch==0.7.0)\n",
            "  Downloading nir-1.0.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nirtorch (from snntorch==0.7.0)\n",
            "  Downloading nirtorch-1.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch==0.7.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch==0.7.0) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch==0.7.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch==0.7.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch==0.7.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch==0.7.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.1.0->snntorch==0.7.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.1.0->snntorch==0.7.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.1.0->snntorch==0.7.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.1.0->snntorch==0.7.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.1.0->snntorch==0.7.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.1.0->snntorch==0.7.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.1.0->snntorch==0.7.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.1.0->snntorch==0.7.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.1.0->snntorch==0.7.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.1.0->snntorch==0.7.0)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.1.0->snntorch==0.7.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch==0.7.0) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.1.0->snntorch==0.7.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch==0.7.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch==0.7.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch==0.7.0) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch==0.7.0) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch==0.7.0) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch==0.7.0) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch==0.7.0) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch==0.7.0) (2.8.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nir->snntorch==0.7.0) (3.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch==0.7.0) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->snntorch==0.7.0) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->snntorch==0.7.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->snntorch==0.7.0) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nir, nvidia-cusolver-cu12, nirtorch, snntorch\n",
            "Successfully installed nir-1.0.1 nirtorch-1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 snntorch-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install binary_fractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rv2l6jUHZO7R",
        "outputId": "38007ceb-12c6-4045-ced5-eda505d359a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting binary_fractions\n",
            "  Downloading binary_fractions-1.1.0.tar.gz (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: binary_fractions\n",
            "  Building wheel for binary_fractions (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for binary_fractions: filename=binary_fractions-1.1.0-py3-none-any.whl size=47920 sha256=588c3db0c60dae480665f188eef4c35ebaaabc6e3ee619a15d55b8e0ebb88c05\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/78/50/5ee188cbaf8195fdaa2d1041c03ae33c9add94ead9131c4713\n",
            "Successfully built binary_fractions\n",
            "Installing collected packages: binary_fractions\n",
            "Successfully installed binary_fractions-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "Sk9yeXHGX8-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_loader(dataset = \"mnist\", in_shape = (28, 28), batch_size = 128):\n",
        "\n",
        "    dtype = torch.float\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "    # Define a transform\n",
        "    transform = transforms.Compose([\n",
        "                transforms.Resize(in_shape),\n",
        "                transforms.Grayscale(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0,), (1,))])\n",
        "\n",
        "    if dataset.lower() == 'mnist':\n",
        "        data_path = './dataset/mnist'\n",
        "        mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
        "        mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
        "        train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "        test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    elif dataset.lower() == 'fashion-mnist':\n",
        "        data_path = './dataset/fashion-mnist'\n",
        "        fmnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transform)\n",
        "        fmnist_test  = datasets.FashionMNIST(data_path, train=False, download=True, transform=transform)\n",
        "        train_loader = DataLoader(fmnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "        test_loader = DataLoader(fmnist_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    elif dataset.lower() == 'cifar10':\n",
        "        data_path = './dataset/cifar10'\n",
        "        cifar10_train = datasets.CIFAR10(data_path, train=True, download=True, transform=transform)\n",
        "        cifar10_test  = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)\n",
        "        train_loader = DataLoader(cifar10_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "        test_loader = DataLoader(cifar10_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "    return train_loader,test_loader"
      ],
      "metadata": {
        "id": "E-psMcR_YHpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = \"fashion-mnist\"\n",
        "train_loader,test_loader = data_loader(dataset = dataset, in_shape = (28, 28), batch_size = 128)"
      ],
      "metadata": {
        "id": "6l66i87iYHsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import snntorch as snn\n",
        "from snntorch import spikeplot as splt\n",
        "from snntorch import spikegen\n",
        "import snntorch.utils as utils\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "x_d14GgiY9x8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_batch_accuracy(net, batch_size, data, targets, train=False):\n",
        "    output, _ = net(data)\n",
        "    _, idx = output.sum(dim=0).max(1)\n",
        "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
        "\n",
        "    if train:\n",
        "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
        "    else:\n",
        "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
        "\n",
        "def train_printer(\n",
        "    net, batch_size,\n",
        "    data, targets, epoch,\n",
        "    counter, iter_counter,\n",
        "        loss_hist, test_loss_hist, test_data, test_targets):\n",
        "    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
        "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
        "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
        "    print('Average Test Loss: ',np.mean(test_loss_hist))\n",
        "    print_batch_accuracy(net, batch_size, data, targets, train=True)\n",
        "    print_batch_accuracy(net, batch_size, test_data, test_targets, train=False)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "g56FFHGsc7VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_inputs=1,\n",
        "                 num_hidden_layers=1,\n",
        "                 num_hidden=1,\n",
        "                 num_outputs=1,\n",
        "                 num_steps=10,\n",
        "                 beta=0.8,\n",
        "                 vth=1.0,\n",
        "                 rst='zero',\n",
        "                 bias=False):\n",
        "        super().__init__()\n",
        "        # Num steps\n",
        "        self.num_steps = num_steps\n",
        "\n",
        "        # Initialize layers\n",
        "        # Generate the model architecture to be simulated\n",
        "        arch = [num_inputs]\n",
        "        for l in range(num_hidden_layers):\n",
        "            arch.append(num_hidden)\n",
        "        arch.append(num_outputs)\n",
        "        # Number of layers\n",
        "        self.num_layers = len(arch) - 1\n",
        "\n",
        "        # Initialize Memory\n",
        "        self.init_mems = list()\n",
        "\n",
        "        # Define Layers\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(self.num_layers):\n",
        "            fc  = nn.Linear(arch[i], arch[i+1], bias=bias)\n",
        "            lif = snn.Leaky(beta=beta,threshold=vth,reset_mechanism=rst)\n",
        "            mem = lif.init_leaky()\n",
        "            self.layers.append(fc)\n",
        "            self.layers.append(lif)\n",
        "            self.init_mems.append(mem)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mems = [self.layers[2*i+1].init_leaky() for i in range(self.num_layers)]\n",
        "\n",
        "        # Record the final layer\n",
        "        spk_out = []\n",
        "        mem_out = []\n",
        "\n",
        "        for step in range(self.num_steps):\n",
        "            spk = x[step]\n",
        "            for i in range(self.num_layers):\n",
        "                cur = self.layers[2*i](spk)\n",
        "                spk,mems[i] = self.layers[2*i+1](cur,mems[i])\n",
        "\n",
        "            spk_out.append(spk)\n",
        "            mem_out.append(mems[-1])\n",
        "\n",
        "        return torch.stack(spk_out, dim=0), torch.stack(mem_out, dim=0)"
      ],
      "metadata": {
        "id": "TDNrlfguZwSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.utils.prune as prune"
      ],
      "metadata": {
        "id": "n0TJCf5La7xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sp in range(10,51,10):\n",
        "\n",
        "  print(f\"Sparsity {sp}%\", \"\\n\\n\")\n",
        "  num_inputs = 28 * 28\n",
        "  num_hidden = 256\n",
        "  num_outputs = 10\n",
        "  num_hidden_layers = 2\n",
        "\n",
        "  dtype = torch.float\n",
        "  device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "  # Load the network onto CUDA if available\n",
        "  net = Net(num_inputs=num_inputs,\n",
        "            num_hidden_layers=num_hidden_layers,\n",
        "            num_hidden=num_hidden,\n",
        "            num_outputs=num_outputs,\n",
        "            num_steps=100,\n",
        "            beta=0.95,\n",
        "            vth=1.0,\n",
        "            rst=\"subtract\").to(device)\n",
        "  # Loss and optimizer\n",
        "  loss = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
        "\n",
        "  # Main training framework starts here\n",
        "  utils.reset(net)\n",
        "  loss_hist = []\n",
        "  test_loss_hist = []\n",
        "  counter = 0\n",
        "  weight_scaling = True\n",
        "  num_epochs = 5\n",
        "  batch_size =128\n",
        "  num_steps = 100\n",
        "\n",
        "  # Pruning Info\n",
        "  parameters_to_prune = []\n",
        "  for name,mod in list(net.named_modules())[1:]:\n",
        "\n",
        "      keys = dict(mod.named_parameters()).keys()\n",
        "\n",
        "      if dict().keys() == keys:\n",
        "          continue\n",
        "      else:\n",
        "          if 'bias' in keys:\n",
        "              parameters_to_prune.append((mod,'bias'))\n",
        "\n",
        "          if 'weight' in keys:\n",
        "              parameters_to_prune.append((mod,'weight'))\n",
        "\n",
        "  print(\"Number of layers to prune: \", len(parameters_to_prune), \"\\n\")\n",
        "\n",
        "  sparsity_amount = sp\n",
        "  percentage_amount = (sparsity_amount/100)\n",
        "  pruning_amount = [percentage_amount  for i in range(len(parameters_to_prune))]\n",
        "  print(pruning_amount, \"\\n\\n\")\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      iter_counter = 0\n",
        "      train_batch = iter(train_loader)\n",
        "\n",
        "      for data, targets in train_batch:\n",
        "          data = spikegen.rate(data.view(batch_size,-1),num_steps=num_steps).to(device)\n",
        "          targets = targets.to(device)\n",
        "          # forward pass\n",
        "          net.train()\n",
        "          spk_rec, mem_rec = net(data)\n",
        "\n",
        "          # initialize the loss & sum over time\n",
        "          loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "          for step in range(num_steps):\n",
        "              loss_val += loss(mem_rec[step], targets)\n",
        "\n",
        "          # Gradient calculation + weight update\n",
        "          optimizer.zero_grad()\n",
        "          loss_val.backward()\n",
        "          optimizer.step()\n",
        "          # Update weights\n",
        "          for model_params in net.parameters():\n",
        "              if weight_scaling:\n",
        "                  model_params.data = torch.clamp(model_params.data,min=-0.9375,max=0.9375)\n",
        "\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "          for j,i in enumerate(parameters_to_prune):\n",
        "              modu,parm = i\n",
        "              try:\n",
        "                  prune.l1_unstructured(modu, name=parm, amount= pruning_amount[j])\n",
        "                  prune.remove(modu, parm)\n",
        "              except Exception:\n",
        "                  pass\n",
        "\n",
        "          # Store loss history for future plotting\n",
        "          loss_hist.append(loss_val.item())\n",
        "          # For inference\n",
        "          # batch_infer = list()\n",
        "          # Test set\n",
        "          with torch.no_grad():\n",
        "              net.eval()\n",
        "              test_data, test_targets = next(iter(test_loader))\n",
        "              test_data = spikegen.rate(test_data.view(batch_size,-1),num_steps=num_steps).to(device)\n",
        "              #test_data = test_data.to(device)\n",
        "              test_targets = test_targets.to(device)\n",
        "              # Test set forward pass\n",
        "              test_spk, test_mem = net(test_data)\n",
        "              # Test set loss\n",
        "              test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
        "              for step in range(num_steps):\n",
        "                  test_loss += loss(test_mem[step], test_targets)\n",
        "              test_loss_hist.append(test_loss.item())\n",
        "              # Print train/test loss/accuracy\n",
        "              if counter % 50 == 0:\n",
        "                  train_printer(\n",
        "                      net, batch_size,\n",
        "                      data, targets, epoch,\n",
        "                      counter, iter_counter,\n",
        "                      loss_hist, test_loss_hist,\n",
        "                      test_data, test_targets)\n",
        "              counter += 1\n",
        "              iter_counter +=1\n",
        "\n",
        "\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "  # Evaluation\n",
        "  pred = []\n",
        "  target = []\n",
        "  net.eval()\n",
        "  for test_data, test_targets in test_loader:\n",
        "    test_data = spikegen.rate(test_data.view(batch_size,-1),num_steps=num_steps).to(device)\n",
        "    test_targets = test_targets\n",
        "    output, _ = net(test_data)\n",
        "    _, idx    = output.sum(dim=0).max(1)\n",
        "\n",
        "    pred += idx.detach().cpu().numpy().tolist()\n",
        "    target += test_targets.numpy().tolist()\n",
        "\n",
        "  acc = np.mean((np.array(target) == np.array(pred)))\n",
        "  print(f\"Test batch accuracy for {sparsity_amount}% sparsity : {acc*100:.2f}%\")\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "  dict_name = dataset + \"_784_256_256_10\" + \"_with_\" + str(sparsity_amount) + \"%_Sparsity\" + '.dict'\n",
        "  model_name = dataset + \"_784_256_256_10\" + \"_with_\" + str(sparsity_amount) + \"%_Sparsity\" +'.mdl'\n",
        "  # Save output\n",
        "  torch.save(net,model_name)\n",
        "  torch.save(net.state_dict(),dict_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9Uh4mFqaRxs",
        "outputId": "421ba621-8292-444e-f478-ca29a4571354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparsity 10% \n",
            "\n",
            "\n",
            "Number of layers to prune:  3 \n",
            "\n",
            "[0.1, 0.1, 0.1] \n",
            "\n",
            "\n",
            "Epoch 0, Iteration 0\n",
            "Train Set Loss: 228.28\n",
            "Test Set Loss: 221.38\n",
            "Average Test Loss:  221.37973022460938\n",
            "Train set accuracy for a single minibatch: 51.56%\n",
            "Test set accuracy for a single minibatch: 33.59%\n",
            "\n",
            "\n",
            "Epoch 0, Iteration 50\n",
            "Train Set Loss: 87.40\n",
            "Test Set Loss: 79.71\n",
            "Average Test Loss:  118.90832010904948\n",
            "Train set accuracy for a single minibatch: 67.19%\n",
            "Test set accuracy for a single minibatch: 62.50%\n",
            "\n",
            "\n",
            "Epoch 0, Iteration 100\n",
            "Train Set Loss: 52.64\n",
            "Test Set Loss: 61.72\n",
            "Average Test Loss:  100.32002209200718\n",
            "Train set accuracy for a single minibatch: 78.12%\n",
            "Test set accuracy for a single minibatch: 79.69%\n",
            "\n",
            "\n",
            "Epoch 0, Iteration 150\n",
            "Train Set Loss: 84.32\n",
            "Test Set Loss: 66.50\n",
            "Average Test Loss:  90.1168429141013\n",
            "Train set accuracy for a single minibatch: 71.88%\n",
            "Test set accuracy for a single minibatch: 75.00%\n",
            "\n",
            "\n",
            "Epoch 0, Iteration 200\n",
            "Train Set Loss: 55.19\n",
            "Test Set Loss: 71.22\n",
            "Average Test Loss:  84.72512720591986\n",
            "Train set accuracy for a single minibatch: 78.91%\n",
            "Test set accuracy for a single minibatch: 71.88%\n",
            "\n",
            "\n",
            "Epoch 0, Iteration 250\n",
            "Train Set Loss: 69.06\n",
            "Test Set Loss: 51.61\n",
            "Average Test Loss:  80.38964451641675\n",
            "Train set accuracy for a single minibatch: 76.56%\n",
            "Test set accuracy for a single minibatch: 73.44%\n",
            "\n",
            "\n",
            "Epoch 0, Iteration 300\n",
            "Train Set Loss: 68.71\n",
            "Test Set Loss: 65.52\n",
            "Average Test Loss:  77.18853294651373\n",
            "Train set accuracy for a single minibatch: 74.22%\n",
            "Test set accuracy for a single minibatch: 60.94%\n",
            "\n",
            "\n",
            "Epoch 0, Iteration 350\n",
            "Train Set Loss: 43.51\n",
            "Test Set Loss: 61.42\n",
            "Average Test Loss:  74.67698690082952\n",
            "Train set accuracy for a single minibatch: 70.31%\n",
            "Test set accuracy for a single minibatch: 72.66%\n",
            "\n",
            "\n",
            "Epoch 0, Iteration 400\n",
            "Train Set Loss: 49.28\n",
            "Test Set Loss: 55.72\n",
            "Average Test Loss:  72.33357952241589\n",
            "Train set accuracy for a single minibatch: 72.66%\n",
            "Test set accuracy for a single minibatch: 78.91%\n",
            "\n",
            "\n",
            "Epoch 0, Iteration 450\n",
            "Train Set Loss: 53.37\n",
            "Test Set Loss: 78.24\n",
            "Average Test Loss:  70.34664576968174\n",
            "Train set accuracy for a single minibatch: 69.53%\n",
            "Test set accuracy for a single minibatch: 69.53%\n",
            "\n",
            "\n",
            "Epoch 1, Iteration 32\n",
            "Train Set Loss: 58.27\n",
            "Test Set Loss: 55.79\n",
            "Average Test Loss:  69.06722095816912\n",
            "Train set accuracy for a single minibatch: 77.34%\n",
            "Test set accuracy for a single minibatch: 71.88%\n",
            "\n",
            "\n",
            "Epoch 1, Iteration 82\n",
            "Train Set Loss: 53.19\n",
            "Test Set Loss: 48.37\n",
            "Average Test Loss:  68.03194224639726\n",
            "Train set accuracy for a single minibatch: 81.25%\n",
            "Test set accuracy for a single minibatch: 78.12%\n",
            "\n",
            "\n",
            "Epoch 1, Iteration 132\n",
            "Train Set Loss: 38.55\n",
            "Test Set Loss: 62.82\n",
            "Average Test Loss:  67.09444346880159\n",
            "Train set accuracy for a single minibatch: 75.78%\n",
            "Test set accuracy for a single minibatch: 83.59%\n",
            "\n",
            "\n",
            "Epoch 1, Iteration 182\n",
            "Train Set Loss: 56.30\n",
            "Test Set Loss: 50.04\n",
            "Average Test Loss:  66.27412130352906\n",
            "Train set accuracy for a single minibatch: 78.12%\n",
            "Test set accuracy for a single minibatch: 83.59%\n",
            "\n",
            "\n",
            "Epoch 1, Iteration 232\n",
            "Train Set Loss: 42.52\n",
            "Test Set Loss: 42.47\n",
            "Average Test Loss:  65.31980371951377\n",
            "Train set accuracy for a single minibatch: 79.69%\n",
            "Test set accuracy for a single minibatch: 80.47%\n",
            "\n",
            "\n",
            "Epoch 1, Iteration 282\n",
            "Train Set Loss: 53.06\n",
            "Test Set Loss: 44.71\n",
            "Average Test Loss:  64.61614842929154\n",
            "Train set accuracy for a single minibatch: 70.31%\n",
            "Test set accuracy for a single minibatch: 75.78%\n",
            "\n",
            "\n",
            "Epoch 1, Iteration 332\n",
            "Train Set Loss: 40.19\n",
            "Test Set Loss: 62.81\n",
            "Average Test Loss:  63.976256757490944\n",
            "Train set accuracy for a single minibatch: 84.38%\n",
            "Test set accuracy for a single minibatch: 76.56%\n",
            "\n",
            "\n",
            "Epoch 1, Iteration 382\n",
            "Train Set Loss: 55.75\n",
            "Test Set Loss: 41.18\n",
            "Average Test Loss:  63.30263151489329\n",
            "Train set accuracy for a single minibatch: 75.78%\n",
            "Test set accuracy for a single minibatch: 79.69%\n",
            "\n",
            "\n",
            "Epoch 1, Iteration 432\n",
            "Train Set Loss: 54.07\n",
            "Test Set Loss: 53.57\n",
            "Average Test Loss:  62.752904396607526\n",
            "Train set accuracy for a single minibatch: 75.00%\n",
            "Test set accuracy for a single minibatch: 72.66%\n",
            "\n",
            "\n",
            "Epoch 2, Iteration 14\n",
            "Train Set Loss: 42.49\n",
            "Test Set Loss: 48.70\n",
            "Average Test Loss:  62.207225093580824\n",
            "Train set accuracy for a single minibatch: 72.66%\n",
            "Test set accuracy for a single minibatch: 71.09%\n",
            "\n",
            "\n",
            "Epoch 2, Iteration 64\n",
            "Train Set Loss: 52.96\n",
            "Test Set Loss: 69.72\n",
            "Average Test Loss:  61.681672331574674\n",
            "Train set accuracy for a single minibatch: 69.53%\n",
            "Test set accuracy for a single minibatch: 64.06%\n",
            "\n",
            "\n",
            "Epoch 2, Iteration 114\n",
            "Train Set Loss: 43.74\n",
            "Test Set Loss: 35.25\n",
            "Average Test Loss:  61.2859091772339\n",
            "Train set accuracy for a single minibatch: 75.78%\n",
            "Test set accuracy for a single minibatch: 68.75%\n",
            "\n",
            "\n",
            "Epoch 2, Iteration 164\n",
            "Train Set Loss: 44.32\n",
            "Test Set Loss: 50.90\n",
            "Average Test Loss:  60.84018547376863\n",
            "Train set accuracy for a single minibatch: 75.78%\n",
            "Test set accuracy for a single minibatch: 75.00%\n",
            "\n",
            "\n",
            "Epoch 2, Iteration 214\n",
            "Train Set Loss: 44.69\n",
            "Test Set Loss: 51.73\n",
            "Average Test Loss:  60.342150604486676\n",
            "Train set accuracy for a single minibatch: 80.47%\n",
            "Test set accuracy for a single minibatch: 70.31%\n",
            "\n",
            "\n",
            "Epoch 2, Iteration 264\n",
            "Train Set Loss: 41.54\n",
            "Test Set Loss: 53.23\n",
            "Average Test Loss:  60.0107920723692\n",
            "Train set accuracy for a single minibatch: 81.25%\n",
            "Test set accuracy for a single minibatch: 70.31%\n",
            "\n",
            "\n",
            "Epoch 2, Iteration 314\n",
            "Train Set Loss: 54.32\n",
            "Test Set Loss: 45.59\n",
            "Average Test Loss:  59.62810084116545\n",
            "Train set accuracy for a single minibatch: 78.91%\n",
            "Test set accuracy for a single minibatch: 78.91%\n",
            "\n",
            "\n",
            "Epoch 2, Iteration 364\n",
            "Train Set Loss: 52.68\n",
            "Test Set Loss: 44.79\n",
            "Average Test Loss:  59.262436061158354\n",
            "Train set accuracy for a single minibatch: 77.34%\n",
            "Test set accuracy for a single minibatch: 75.78%\n",
            "\n",
            "\n",
            "Epoch 2, Iteration 414\n",
            "Train Set Loss: 50.11\n",
            "Test Set Loss: 54.37\n",
            "Average Test Loss:  58.961625978030774\n",
            "Train set accuracy for a single minibatch: 74.22%\n",
            "Test set accuracy for a single minibatch: 77.34%\n",
            "\n",
            "\n",
            "Epoch 2, Iteration 464\n",
            "Train Set Loss: 33.62\n",
            "Test Set Loss: 32.18\n",
            "Average Test Loss:  58.58487605708229\n",
            "Train set accuracy for a single minibatch: 80.47%\n",
            "Test set accuracy for a single minibatch: 76.56%\n",
            "\n",
            "\n",
            "Epoch 3, Iteration 46\n",
            "Train Set Loss: 50.04\n",
            "Test Set Loss: 51.71\n",
            "Average Test Loss:  58.279932458510814\n",
            "Train set accuracy for a single minibatch: 78.12%\n",
            "Test set accuracy for a single minibatch: 81.25%\n",
            "\n",
            "\n",
            "Epoch 3, Iteration 96\n",
            "Train Set Loss: 37.21\n",
            "Test Set Loss: 45.95\n",
            "Average Test Loss:  57.903270521932726\n",
            "Train set accuracy for a single minibatch: 79.69%\n",
            "Test set accuracy for a single minibatch: 74.22%\n",
            "\n",
            "\n",
            "Epoch 3, Iteration 146\n",
            "Train Set Loss: 35.06\n",
            "Test Set Loss: 41.33\n",
            "Average Test Loss:  57.57882898135158\n",
            "Train set accuracy for a single minibatch: 71.88%\n",
            "Test set accuracy for a single minibatch: 73.44%\n",
            "\n",
            "\n",
            "Epoch 3, Iteration 196\n",
            "Train Set Loss: 67.41\n",
            "Test Set Loss: 41.07\n",
            "Average Test Loss:  57.29119130956017\n",
            "Train set accuracy for a single minibatch: 71.09%\n",
            "Test set accuracy for a single minibatch: 71.88%\n",
            "\n",
            "\n",
            "Epoch 3, Iteration 246\n",
            "Train Set Loss: 43.22\n",
            "Test Set Loss: 38.14\n",
            "Average Test Loss:  57.06596321831466\n",
            "Train set accuracy for a single minibatch: 81.25%\n",
            "Test set accuracy for a single minibatch: 89.06%\n",
            "\n",
            "\n",
            "Epoch 3, Iteration 296\n",
            "Train Set Loss: 30.73\n",
            "Test Set Loss: 44.34\n",
            "Average Test Loss:  56.82461536189937\n",
            "Train set accuracy for a single minibatch: 85.16%\n",
            "Test set accuracy for a single minibatch: 82.81%\n",
            "\n",
            "\n",
            "Epoch 3, Iteration 346\n",
            "Train Set Loss: 32.87\n",
            "Test Set Loss: 40.72\n",
            "Average Test Loss:  56.561545558822694\n",
            "Train set accuracy for a single minibatch: 79.69%\n",
            "Test set accuracy for a single minibatch: 75.78%\n",
            "\n",
            "\n",
            "Epoch 3, Iteration 396\n",
            "Train Set Loss: 33.65\n",
            "Test Set Loss: 55.50\n",
            "Average Test Loss:  56.324771885339715\n",
            "Train set accuracy for a single minibatch: 76.56%\n",
            "Test set accuracy for a single minibatch: 74.22%\n",
            "\n",
            "\n",
            "Epoch 3, Iteration 446\n",
            "Train Set Loss: 39.56\n",
            "Test Set Loss: 45.06\n",
            "Average Test Loss:  56.04228653642437\n",
            "Train set accuracy for a single minibatch: 83.59%\n",
            "Test set accuracy for a single minibatch: 78.12%\n",
            "\n",
            "\n",
            "Epoch 4, Iteration 28\n",
            "Train Set Loss: 42.77\n",
            "Test Set Loss: 59.19\n",
            "Average Test Loss:  55.89310076059886\n",
            "Train set accuracy for a single minibatch: 78.91%\n",
            "Test set accuracy for a single minibatch: 76.56%\n",
            "\n",
            "\n",
            "Epoch 4, Iteration 78\n",
            "Train Set Loss: 35.42\n",
            "Test Set Loss: 39.58\n",
            "Average Test Loss:  55.661779748788554\n",
            "Train set accuracy for a single minibatch: 84.38%\n",
            "Test set accuracy for a single minibatch: 88.28%\n",
            "\n",
            "\n",
            "Epoch 4, Iteration 128\n",
            "Train Set Loss: 32.84\n",
            "Test Set Loss: 42.89\n",
            "Average Test Loss:  55.52933698555996\n",
            "Train set accuracy for a single minibatch: 85.16%\n",
            "Test set accuracy for a single minibatch: 78.91%\n",
            "\n",
            "\n",
            "Epoch 4, Iteration 178\n",
            "Train Set Loss: 65.64\n",
            "Test Set Loss: 58.90\n",
            "Average Test Loss:  55.34053993225098\n",
            "Train set accuracy for a single minibatch: 81.25%\n",
            "Test set accuracy for a single minibatch: 75.00%\n",
            "\n",
            "\n",
            "Epoch 4, Iteration 228\n",
            "Train Set Loss: 38.79\n",
            "Test Set Loss: 31.94\n",
            "Average Test Loss:  55.08962190565411\n",
            "Train set accuracy for a single minibatch: 78.91%\n",
            "Test set accuracy for a single minibatch: 80.47%\n",
            "\n",
            "\n",
            "Epoch 4, Iteration 278\n",
            "Train Set Loss: 32.26\n",
            "Test Set Loss: 42.15\n",
            "Average Test Loss:  54.91619257215343\n",
            "Train set accuracy for a single minibatch: 86.72%\n",
            "Test set accuracy for a single minibatch: 85.16%\n",
            "\n",
            "\n",
            "Epoch 4, Iteration 328\n",
            "Train Set Loss: 42.62\n",
            "Test Set Loss: 43.67\n",
            "Average Test Loss:  54.77697956773272\n",
            "Train set accuracy for a single minibatch: 75.00%\n",
            "Test set accuracy for a single minibatch: 78.12%\n",
            "\n",
            "\n",
            "Epoch 4, Iteration 378\n",
            "Train Set Loss: 31.72\n",
            "Test Set Loss: 40.75\n",
            "Average Test Loss:  54.63786745939928\n",
            "Train set accuracy for a single minibatch: 85.94%\n",
            "Test set accuracy for a single minibatch: 78.91%\n",
            "\n",
            "\n",
            "Epoch 4, Iteration 428\n",
            "Train Set Loss: 30.78\n",
            "Test Set Loss: 59.78\n",
            "Average Test Loss:  54.427466969860994\n",
            "Train set accuracy for a single minibatch: 79.69%\n",
            "Test set accuracy for a single minibatch: 73.44%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Test batch accuracy for 10% sparsity : 80.86%\n",
            "\n",
            "\n",
            "\n",
            "Sparsity 20% \n",
            "\n",
            "\n",
            "Number of layers to prune:  3 \n",
            "\n",
            "[0.2, 0.2, 0.2] \n",
            "\n",
            "\n",
            "Epoch 0, Iteration 0\n",
            "Train Set Loss: 240.97\n",
            "Test Set Loss: 213.93\n",
            "Average Test Loss:  213.93397521972656\n",
            "Train set accuracy for a single minibatch: 29.69%\n",
            "Test set accuracy for a single minibatch: 24.22%\n",
            "\n",
            "\n",
            "Epoch 0, Iteration 50\n",
            "Train Set Loss: 89.82\n",
            "Test Set Loss: 83.99\n",
            "Average Test Loss:  120.13100762460746\n",
            "Train set accuracy for a single minibatch: 71.88%\n",
            "Test set accuracy for a single minibatch: 71.09%\n",
            "\n",
            "\n",
            "Epoch 0, Iteration 100\n",
            "Train Set Loss: 74.00\n",
            "Test Set Loss: 84.04\n",
            "Average Test Loss:  99.6843182780955\n",
            "Train set accuracy for a single minibatch: 80.47%\n",
            "Test set accuracy for a single minibatch: 75.78%\n",
            "\n",
            "\n",
            "Epoch 0, Iteration 150\n",
            "Train Set Loss: 62.47\n",
            "Test Set Loss: 43.85\n",
            "Average Test Loss:  88.29963524925788\n",
            "Train set accuracy for a single minibatch: 76.56%\n",
            "Test set accuracy for a single minibatch: 73.44%\n",
            "\n",
            "\n",
            "Epoch 0, Iteration 200\n",
            "Train Set Loss: 57.29\n",
            "Test Set Loss: 69.02\n",
            "Average Test Loss:  83.32345190095664\n",
            "Train set accuracy for a single minibatch: 74.22%\n",
            "Test set accuracy for a single minibatch: 70.31%\n",
            "\n",
            "\n",
            "Epoch 0, Iteration 250\n",
            "Train Set Loss: 55.95\n",
            "Test Set Loss: 72.24\n",
            "Average Test Loss:  79.19732143204526\n",
            "Train set accuracy for a single minibatch: 71.88%\n",
            "Test set accuracy for a single minibatch: 74.22%\n",
            "\n",
            "\n",
            "Epoch 0, Iteration 300\n",
            "Train Set Loss: 65.67\n",
            "Test Set Loss: 63.61\n",
            "Average Test Loss:  75.61459880334594\n",
            "Train set accuracy for a single minibatch: 75.78%\n",
            "Test set accuracy for a single minibatch: 76.56%\n",
            "\n",
            "\n",
            "Epoch 0, Iteration 350\n",
            "Train Set Loss: 47.15\n",
            "Test Set Loss: 56.28\n",
            "Average Test Loss:  73.38864467218731\n",
            "Train set accuracy for a single minibatch: 82.81%\n",
            "Test set accuracy for a single minibatch: 78.91%\n",
            "\n",
            "\n",
            "Epoch 0, Iteration 400\n",
            "Train Set Loss: 66.97\n",
            "Test Set Loss: 52.81\n",
            "Average Test Loss:  71.49089850332969\n",
            "Train set accuracy for a single minibatch: 67.19%\n",
            "Test set accuracy for a single minibatch: 74.22%\n",
            "\n",
            "\n",
            "Epoch 0, Iteration 450\n",
            "Train Set Loss: 50.87\n",
            "Test Set Loss: 55.64\n",
            "Average Test Loss:  70.14896133152186\n",
            "Train set accuracy for a single minibatch: 75.00%\n",
            "Test set accuracy for a single minibatch: 75.78%\n",
            "\n",
            "\n",
            "Epoch 1, Iteration 32\n",
            "Train Set Loss: 62.74\n",
            "Test Set Loss: 46.81\n",
            "Average Test Loss:  68.51228696476676\n",
            "Train set accuracy for a single minibatch: 69.53%\n",
            "Test set accuracy for a single minibatch: 84.38%\n",
            "\n",
            "\n",
            "Epoch 1, Iteration 82\n",
            "Train Set Loss: 72.01\n",
            "Test Set Loss: 71.27\n",
            "Average Test Loss:  67.43913527279281\n",
            "Train set accuracy for a single minibatch: 72.66%\n",
            "Test set accuracy for a single minibatch: 71.88%\n",
            "\n",
            "\n",
            "Epoch 1, Iteration 132\n",
            "Train Set Loss: 67.14\n",
            "Test Set Loss: 81.27\n",
            "Average Test Loss:  66.31761295109145\n",
            "Train set accuracy for a single minibatch: 71.09%\n",
            "Test set accuracy for a single minibatch: 75.78%\n",
            "\n",
            "\n",
            "Epoch 1, Iteration 182\n",
            "Train Set Loss: 49.41\n",
            "Test Set Loss: 41.65\n",
            "Average Test Loss:  65.3948538907662\n",
            "Train set accuracy for a single minibatch: 72.66%\n",
            "Test set accuracy for a single minibatch: 74.22%\n",
            "\n",
            "\n",
            "Epoch 1, Iteration 232\n",
            "Train Set Loss: 38.63\n",
            "Test Set Loss: 53.97\n",
            "Average Test Loss:  64.69333898187874\n",
            "Train set accuracy for a single minibatch: 78.91%\n",
            "Test set accuracy for a single minibatch: 76.56%\n",
            "\n",
            "\n",
            "Epoch 1, Iteration 282\n",
            "Train Set Loss: 54.94\n",
            "Test Set Loss: 52.64\n",
            "Average Test Loss:  63.901534696393576\n",
            "Train set accuracy for a single minibatch: 74.22%\n",
            "Test set accuracy for a single minibatch: 75.00%\n",
            "\n",
            "\n",
            "Epoch 1, Iteration 332\n",
            "Train Set Loss: 59.40\n",
            "Test Set Loss: 55.89\n",
            "Average Test Loss:  63.30064825231812\n",
            "Train set accuracy for a single minibatch: 76.56%\n",
            "Test set accuracy for a single minibatch: 75.78%\n",
            "\n",
            "\n",
            "Epoch 1, Iteration 382\n",
            "Train Set Loss: 66.45\n",
            "Test Set Loss: 56.82\n",
            "Average Test Loss:  62.5707036454305\n",
            "Train set accuracy for a single minibatch: 76.56%\n",
            "Test set accuracy for a single minibatch: 82.03%\n",
            "\n",
            "\n",
            "Epoch 1, Iteration 432\n",
            "Train Set Loss: 46.44\n",
            "Test Set Loss: 53.22\n",
            "Average Test Loss:  62.15909370227606\n",
            "Train set accuracy for a single minibatch: 82.81%\n",
            "Test set accuracy for a single minibatch: 76.56%\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7gVUQimxaR0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To check sparsity"
      ],
      "metadata": {
        "id": "vReyMmPfi7kA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.load(\"MNIST/mnist_784_256_256_10\" + \"_with_\" + str(20) + \"%_Sparsity\" + '.dict', map_location=torch.device('cpu'))"
      ],
      "metadata": {
        "id": "ijmXZIKoaR2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w.keys()"
      ],
      "metadata": {
        "id": "fGbVdFdOaR5z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d4b6954-8324-4619-f2da-97c48340f9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['layers.0.weight', 'layers.1.threshold', 'layers.1.graded_spikes_factor', 'layers.1.reset_mechanism_val', 'layers.1.beta', 'layers.2.weight', 'layers.3.threshold', 'layers.3.graded_spikes_factor', 'layers.3.reset_mechanism_val', 'layers.3.beta', 'layers.4.weight', 'layers.5.threshold', 'layers.5.graded_spikes_factor', 'layers.5.reset_mechanism_val', 'layers.5.beta'])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "totParams = 0\n",
        "sparseParams = 0\n",
        "for ix,i in enumerate([w['layers.0.weight'], w['layers.2.weight'], w['layers.4.weight']]):\n",
        "  var = (i !=0).sum()\n",
        "  print(\"Linear Layer \",ix,\": \",var, i.shape, \"Params :\", i.shape[0] * i.shape[1])\n",
        "  sparseParams += var.item()\n",
        "  totParams += i.shape[0] * i.shape[1]\n",
        "\n",
        "print(f\"\\nTotal Params : {totParams}\")\n",
        "print(f\"Sparse Params : {sparseParams}\")"
      ],
      "metadata": {
        "id": "WWbIsxjlaR8n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff9b161f-4ef7-4d5b-ca99-e61b50bd5cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Layer  0 :  tensor(160563) torch.Size([256, 784]) Params : 200704\n",
            "Linear Layer  1 :  tensor(52429) torch.Size([256, 256]) Params : 65536\n",
            "Linear Layer  2 :  tensor(2048) torch.Size([10, 256]) Params : 2560\n",
            "\n",
            "Total Params : 268800\n",
            "Sparse Params : 215040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sparsity\n",
        "\n",
        "print(f\"Sparsity : {(1 - sparseParams/totParams)*100:.2f}%\")"
      ],
      "metadata": {
        "id": "l0ApKqMoaR_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ccc66eb-1aad-4083-b066-eb89357db73a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparsity : 20.00%\n"
          ]
        }
      ]
    }
  ]
}